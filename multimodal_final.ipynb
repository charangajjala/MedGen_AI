{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:56:27.449183Z",
     "iopub.status.busy": "2024-07-20T04:56:27.448535Z",
     "iopub.status.idle": "2024-07-20T04:56:40.829787Z",
     "shell.execute_reply": "2024-07-20T04:56:40.828684Z",
     "shell.execute_reply.started": "2024-07-20T04:56:27.449149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece transformers[sentencepiece] accelerate datasets peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:56:40.832067Z",
     "iopub.status.busy": "2024-07-20T04:56:40.831708Z",
     "iopub.status.idle": "2024-07-20T04:56:43.869802Z",
     "shell.execute_reply": "2024-07-20T04:56:43.868856Z",
     "shell.execute_reply.started": "2024-07-20T04:56:40.832029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Login to Hugging Face\n",
    "# login(token=\"YOUR_HF_TOKEN_HERE\")  # Uncomment and add your token when running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:56:43.871420Z",
     "iopub.status.busy": "2024-07-20T04:56:43.871008Z",
     "iopub.status.idle": "2024-07-20T04:56:51.959751Z",
     "shell.execute_reply": "2024-07-20T04:56:51.958757Z",
     "shell.execute_reply.started": "2024-07-20T04:56:43.871394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 04:56:46.787619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-20 04:56:46.787683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-20 04:56:46.789180: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:26:26.324473Z",
     "iopub.status.busy": "2024-07-20T05:26:26.324041Z",
     "iopub.status.idle": "2024-07-20T05:26:26.363148Z",
     "shell.execute_reply": "2024-07-20T05:26:26.362182Z",
     "shell.execute_reply.started": "2024-07-20T05:26:26.324440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 8\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 8\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# # Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 20\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = False\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    logging_strategy=\"epoch\",     # Log at the end of every epoch\n",
    "    report_to=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:56:52.029745Z",
     "iopub.status.busy": "2024-07-20T04:56:52.029448Z",
     "iopub.status.idle": "2024-07-20T04:56:52.997026Z",
     "shell.execute_reply": "2024-07-20T04:56:52.996212Z",
     "shell.execute_reply.started": "2024-07-20T04:56:52.029720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.densenet import DenseNet121_Weights\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_features, num_diseases, num_patches):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(num_diseases, num_patches, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_weights = torch.softmax(self.attention_weights, dim=1)\n",
    "        attention_weights = attention_weights.permute(1, 0, 2)\n",
    "        attended_features = torch.einsum('bpc,pdc->bdc', x, attention_weights) \n",
    "        return attended_features, attention_weights\n",
    "\n",
    "\n",
    "class DenseNet121WithAttention(nn.Module):\n",
    "    def __init__(self, out_size, num_diseases):\n",
    "        super(DenseNet121WithAttention, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.vision_feature_dim = num_ftrs\n",
    "        self.densenet121.classifier = nn.Identity()\n",
    "        # self.freeze_densenet121()\n",
    "        \n",
    "        self.attention_module = AttentionModule(num_ftrs, num_diseases, num_patches=49)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5), \n",
    "            nn.Linear(num_diseases * num_ftrs, out_size),  #logits\n",
    "        )\n",
    "        self.gradients = None\n",
    "\n",
    "    def freeze_densenet121(self):\n",
    "        for param in self.densenet121.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def save_gradients(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.densenet121.features(x)\n",
    "        features = features.view(features.size(0), features.size(1), -1).permute(0, 2, 1)\n",
    "        attended_features, attention_weights = self.attention_module(features)\n",
    "        out = attended_features.reshape(attended_features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        if self.training and features.requires_grad:\n",
    "            features.register_hook(self.save_gradients)\n",
    "        \n",
    "        return out, attention_weights, attended_features\n",
    "    \n",
    "# Check if a GPU is available, otherwise use the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the model\n",
    "vision_model = DenseNet121WithAttention(out_size=14, num_diseases=14)\n",
    "\n",
    "# Load the saved model parameters\n",
    "vision_model.load_state_dict(torch.load('/kaggle/input/x-ray-dataset-new/dense_net_121_d_e50.pth'))\n",
    "\n",
    "vision_model.to(device)\n",
    "\n",
    "# Set the vision_model to evaluation mode if you're making predictions\n",
    "vision_model.eval()\n",
    "\n",
    "visual_feature_dim = vision_model.vision_feature_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:56:52.998803Z",
     "iopub.status.busy": "2024-07-20T04:56:52.998424Z",
     "iopub.status.idle": "2024-07-20T04:57:00.099346Z",
     "shell.execute_reply": "2024-07-20T04:57:00.098302Z",
     "shell.execute_reply.started": "2024-07-20T04:56:52.998770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c42e758f704738b9836d87f8048d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "llm_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    quantization_config=bnb_config\n",
    "#     device_map=device_map  # This will automatically assign layers to GPUs if available\n",
    ")\n",
    "llm_model.config.use_cache = False\n",
    "llm_model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name, trust_remote_code=True)\n",
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "\n",
    "llm_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:57:00.100981Z",
     "iopub.status.busy": "2024-07-20T04:57:00.100650Z",
     "iopub.status.idle": "2024-07-20T04:57:00.383794Z",
     "shell.execute_reply": "2024-07-20T04:57:00.382726Z",
     "shell.execute_reply.started": "2024-07-20T04:57:00.100952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm_model = get_peft_model(llm_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:27:30.711376Z",
     "iopub.status.busy": "2024-07-20T05:27:30.710665Z",
     "iopub.status.idle": "2024-07-20T05:27:30.742036Z",
     "shell.execute_reply": "2024-07-20T05:27:30.741235Z",
     "shell.execute_reply.started": "2024-07-20T05:27:30.711334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_diseases = 14\n",
    "# Custom dataset class\n",
    "class PreCarDiv_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_data, tokenizer, vision_model,transform):\n",
    "        self.csv_data = csv_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vision_model = vision_model\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.csv_data[idx]['image_path']\n",
    "        image_path = os.path.join('/kaggle/input/x-ray-dataset-new/archive/Chexpertplus_Images/preprocessed_images2', image_path) \n",
    "        diagnostic_prompt = self.csv_data[idx]['diagnostic_prompt']\n",
    "        target_report = self.csv_data[idx]['target_report']\n",
    "\n",
    "        image = self.load_image(image_path)\n",
    "        image = self.transform(image).unsqueeze(0).to(device)  # Move image to the same device as the model\n",
    "        \n",
    "        _,_,visual_features = self.vision_model(image)\n",
    "\n",
    "        visual_features = visual_features.view(1 * num_diseases, -1).cpu()\n",
    "\n",
    "        target_report_id = tokenizer(target_report, return_tensors='pt').input_ids.squeeze(0)\n",
    "         # Tokenize the diagnostic prompt and remove the batch dimension\n",
    "        diagnostic_prompt_ids = self.tokenizer(diagnostic_prompt, return_tensors='pt').input_ids.squeeze(0)\n",
    "        diagnostic_prompt_len =diagnostic_prompt_ids.shape[0]\n",
    "        \n",
    "        # Concatenate the token IDs\n",
    "        combined_input_id = torch.cat((diagnostic_prompt_ids, target_report_id), dim=0)\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        combined_input_id = torch.cat((combined_input_id, torch.tensor([eos_token_id], dtype=torch.long)), dim=0)\n",
    "\n",
    "        # Calculate lengths\n",
    "        visual_tokens_len = visual_features.shape[0]  # Number of visual tokens\n",
    "\n",
    "\n",
    "        # Create labels with padding and masking for non-target tokens\n",
    "        labels = torch.full((visual_tokens_len + combined_input_id.shape[0],), -100, dtype=torch.long)\n",
    "\n",
    "        # Determine start index for the target report within the combined sequence\n",
    "        target_start_idx = visual_tokens_len + diagnostic_prompt_len\n",
    "        \n",
    "         # Set the labels for the target text part, shifted by one position to the right\n",
    "        labels[target_start_idx:target_start_idx + len(target_report_id) - 1] = target_report_id[1:]\n",
    "        labels[target_start_idx + len(target_report_id) - 1] = eos_token_id # Ensure the last token predicts EOS\n",
    "        \n",
    "#         print(\"Lables\", labels)\n",
    "#         print('Sample:', idx)\n",
    "#         print(f\"visual_tokens_len: {visual_tokens_len}\")\n",
    "#         print(f\"target_report_ids.shape: {target_report_id.shape}\")\n",
    "#         print(f\"diagnostic_prompt_ids.shape: {diagnostic_prompt_ids.shape}\")\n",
    "#         print(f\"combined_input_ids.shape: {combined_input_id.shape}\")\n",
    "#         print(f\"target_start_idx: {target_start_idx}\")\n",
    "#         print(f\"target_report_id[1:].shape: {target_report_id[1:].shape}\")\n",
    "#         print(f\"Final labels.shape: {labels.shape}\")\n",
    "#         print(f'Visual Features from dataset: {visual_features.shape}')\n",
    "#         print('#'*20)\n",
    "\n",
    "        return {\n",
    "            'visual_features': visual_features,\n",
    "            'input_ids': combined_input_id,\n",
    "            'attention_mask': torch.cat((\n",
    "                torch.ones(visual_tokens_len, dtype=torch.long),\n",
    "                torch.ones(len(combined_input_id), dtype=torch.long)\n",
    "            )),\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, padding_value):\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences, padding_value=padding_value)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "#     print(\"Batch shape\",len(batch))\n",
    "    visual_features = torch.stack([item['visual_features'] for item in batch])\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], padding_value=tokenizer.pad_token_id).transpose(0, 1)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], padding_value=0).transpose(0, 1)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], padding_value=-100).transpose(0, 1)\n",
    "    \n",
    "#     print(\"Lables shape\",labels.shape)\n",
    "    \n",
    "#     print(\"Input Ids\", input_ids)\n",
    "#     print(\"A_Maks\",attention_mask)\n",
    "#     print(\"labels\",labels)\n",
    "\n",
    "    return {\n",
    "        'visual_features': visual_features,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "# Load your CSV data using pandas\n",
    "csv_file_path = '/kaggle/input/x-ray-dataset-new/final_dataset_latest.csv'\n",
    "df = pd.read_csv(csv_file_path,nrows=100)\n",
    "# Convert the dataframe to a list of dictionaries\n",
    "csv_data = df.to_dict(orient='records')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Splitting the data into train, validation, and test sets in 60-20-20 ratio\n",
    "train_val_data, test_data = train_test_split(csv_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "train_dataset = PreCarDiv_Dataset(train_data, tokenizer, vision_model, transform)\n",
    "test_dataset = PreCarDiv_Dataset(test_data, tokenizer, vision_model, transform)\n",
    "val_dataset = PreCarDiv_Dataset(val_data, tokenizer, vision_model, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:57:00.421999Z",
     "iopub.status.busy": "2024-07-20T04:57:00.421667Z",
     "iopub.status.idle": "2024-07-20T04:57:00.506285Z",
     "shell.execute_reply": "2024-07-20T04:57:00.505443Z",
     "shell.execute_reply.started": "2024-07-20T04:57:00.421972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model_PreCarDiv(nn.Module):\n",
    "    def __init__(self, visual_feature_dim, llm_model):\n",
    "        super(Model_PreCarDiv, self).__init__()\n",
    "        self.llm_model = llm_model\n",
    "        self.visual_projection = nn.Linear(visual_feature_dim, llm_model.config.hidden_size).to(llm_model.dtype)\n",
    "\n",
    "    def forward(self, visual_features, input_ids, attention_mask, labels=None):\n",
    "        visual_features, combined_embeddings = self.prepare_features(visual_features, input_ids)\n",
    "        outputs = self.llm_model(inputs_embeds=combined_embeddings, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "    def prepare_features(self, visual_features, input_ids):\n",
    "        # Convert visual_features to the same dtype as model parameters\n",
    "        visual_features = visual_features.to(self.llm_model.dtype)\n",
    "                                           \n",
    "        # Visual_features shape is (batch_size, num_diseases, visual_feature_dim)\n",
    "        batch_size, num_diseases, _ = visual_features.shape\n",
    "        \n",
    "        # Reshape visual features to (batch_size * num_diseases, visual_feature_dim)\n",
    "        visual_features_reshaped = visual_features.view(batch_size * num_diseases, -1)\n",
    "        \n",
    "        # Project visual features to the same dimension as text embeddings\n",
    "        visual_embeddings_reshaped = self.visual_projection(visual_features_reshaped)\n",
    "        \n",
    "        # Reshape back to (batch_size, num_diseases, hidden_size)\n",
    "        visual_embeddings = visual_embeddings_reshaped.view(batch_size, num_diseases, -1)\n",
    "        \n",
    "        # Get the text embeddings from the model's embedding layer (batch_size, seq_len, hidden_size)\n",
    "        text_embeddings = self.llm_model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # Combine visual and text embeddings  (batch_size, num_diseases+seq_len, hidden_size)\n",
    "        combined_embeddings = torch.cat((visual_embeddings, text_embeddings), dim=1)\n",
    "        \n",
    "        return visual_features, combined_embeddings\n",
    "\n",
    "    def generate(self, visual_features, input_ids, attention_mask, max_length=512, num_beams=5, early_stopping=True, no_repeat_ngram_size=2):\n",
    "        self.llm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, combined_embeddings = self.prepare_features(visual_features, input_ids)\n",
    "            # Start with given input_ids and expand generation up to max_length\n",
    "            generated_ids = self.llm_model.generate(\n",
    "                inputs_embeds=combined_embeddings,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=early_stopping,\n",
    "                no_repeat_ngram_size=2,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        return generated_ids\n",
    "\n",
    "    \n",
    "multi_model = Model_PreCarDiv(visual_feature_dim,llm_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T04:57:00.509266Z",
     "iopub.status.busy": "2024-07-20T04:57:00.508962Z",
     "iopub.status.idle": "2024-07-20T04:57:00.522389Z",
     "shell.execute_reply": "2024-07-20T04:57:00.521367Z",
     "shell.execute_reply.started": "2024-07-20T04:57:00.509241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 12587008\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_trainable_params = count_parameters(multi_model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:27:34.625001Z",
     "iopub.status.busy": "2024-07-20T05:27:34.624592Z",
     "iopub.status.idle": "2024-07-20T05:27:58.514190Z",
     "shell.execute_reply": "2024-07-20T05:27:58.512758Z",
     "shell.execute_reply.started": "2024-07-20T05:27:34.624969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Custom trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        visual_features = inputs['visual_features'].to(device)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = inputs['labels'].to(device)\n",
    "        outputs = model(visual_features, input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = CustomTrainer(\n",
    "    model=multi_model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_arguments,\n",
    "    data_collator=custom_collate_fn,\n",
    "    eval_dataset = val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:11:36.190749Z",
     "iopub.status.busy": "2024-07-20T05:11:36.190375Z",
     "iopub.status.idle": "2024-07-20T05:11:36.200555Z",
     "shell.execute_reply": "2024-07-20T05:11:36.199625Z",
     "shell.execute_reply.started": "2024-07-20T05:11:36.190717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions(data_loader, model, tokenizer, device, max_length=512):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            visual_features = batch['visual_features'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Generate sequences using the model's generate method\n",
    "            generated_ids = model.generate(\n",
    "                visual_features=visual_features,\n",
    "                attention_mask=attention_mask,\n",
    "                input_ids=input_ids,\n",
    "                num_beams=5,            # Use beam search with specified number of beams\n",
    "                early_stopping=True,    # Stop generating as soon as all beams are finished\n",
    "                no_repeat_ngram_size=2,  # Prevent repeating n-grams,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the generated ids and the labels to text\n",
    "            for i, gen_ids in enumerate(generated_ids):\n",
    "                predicted_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                reference_text = tokenizer.decode(labels[i][labels[i] != -100], skip_special_tokens=True)  # filtering out -100 used for ignored indices\n",
    "                print('Reference Report', reference_text)\n",
    "                print('Predicted Report', predicted_text)\n",
    "                predictions.append(predicted_text)\n",
    "                references.append([reference_text])  # references must be a list of lists for sacrebleu\n",
    "\n",
    "    return predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:11:57.038032Z",
     "iopub.status.busy": "2024-07-20T05:11:57.037679Z",
     "iopub.status.idle": "2024-07-20T05:16:55.259541Z",
     "shell.execute_reply": "2024-07-20T05:16:55.258583Z",
     "shell.execute_reply.started": "2024-07-20T05:11:57.038000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bleu_metric = BLEU()\n",
    "\n",
    "# Create a DataLoader for your evaluation dataset\n",
    "data_loader = DataLoader(val_dataset, batch_size=2, collate_fn=custom_collate_fn)\n",
    "predictions, references = generate_predictions(data_loader, multi_model, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T05:16:55.261545Z",
     "iopub.status.busy": "2024-07-20T05:16:55.260921Z",
     "iopub.status.idle": "2024-07-20T05:16:55.274557Z",
     "shell.execute_reply": "2024-07-20T05:16:55.273706Z",
     "shell.execute_reply.started": "2024-07-20T05:16:55.261518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate the BLEU score\n",
    "bleu_score = bleu_metric.corpus_score(predictions, references)\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5379261,
     "sourceId": 8986222,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
